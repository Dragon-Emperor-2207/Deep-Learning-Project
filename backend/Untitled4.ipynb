{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQKcWnGeC_IY",
        "outputId": "922451b4-ac53-4242-cb20-db82417134a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n",
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['real', 'stand', 'good']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency 1: ['real', 'stand', 'good']\n",
            "{'title_good': \"the storyline lacked any real tension, but didn't really care.\", 'title_bad': '. -', 'good_summary': \" : . :: ! !! .. .! ? !. & ! # ! The storyline lacked any real tension, but I didn't really care . In a lot of ways, it is a set up for the rest of the series, and definitely not a standalone movie . This is a fun movie, it ties two TV shows into the lore and does a good job of having fun . If you get over its not adding anything or revealing any easter eggs, you can enjoy it for what it is . It improves on captain marvels solo outing .\", 'bad_summary': ''}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adjectives': defaultdict(list, {1: ['real', 'stand', 'good']}),\n",
              " 'summaries': {'title_good': \"the storyline lacked any real tension, but didn't really care.\",\n",
              "  'title_bad': '. -',\n",
              "  'good_summary': \" : . :: ! !! .. .! ? !. & ! # ! The storyline lacked any real tension, but I didn't really care . In a lot of ways, it is a set up for the rest of the series, and definitely not a standalone movie . This is a fun movie, it ties two TV shows into the lore and does a good job of having fun . If you get over its not adding anything or revealing any easter eggs, you can enjoy it for what it is . It improves on captain marvels solo outing .\",\n",
              "  'bad_summary': ''}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def main():\n",
        "  from transformers import pipeline\n",
        "  import tensorflow as tf\n",
        "  from transformers import AutoTokenizer, TFAutoModelForCausalLM, TFAutoModelForSequenceClassification, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  file_path1 = '/content/summary1.txt'\n",
        "  file_path2 = '/content/summary2.txt'\n",
        "  file_path3 = '/content/summary3.txt'\n",
        "\n",
        "  final_summary_good = []\n",
        "  final_summary_bad = []\n",
        "  neutral_sammary = ''\n",
        "\n",
        "  sentiment_input = []\n",
        "  sentiment_summary = []\n",
        "\n",
        "  summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "  def summary_generator(text,n):\n",
        "\n",
        "    array= ''\n",
        "    for i in text:\n",
        "      summary = summarizer(i, max_length=130, min_length=30, do_sample=False)\n",
        "      m=summary[0]['summary_text']+ ' '\n",
        "\n",
        "      if n==1:\n",
        "        final_summary_good.append(m)\n",
        "\n",
        "      elif n==2:\n",
        "        final_summary_bad.append(m)\n",
        "\n",
        "      else:\n",
        "        sentiment_summary.append(m)\n",
        "        sentiment_input.append(i)\n",
        "\n",
        "  # Initialize the summarization pipeline\n",
        "\n",
        "  with open(file_path1, 'r') as file:\n",
        "      text1 = file.read()\n",
        "\n",
        "  with open(file_path2, 'r') as file:\n",
        "      text2 = file.read()\n",
        "\n",
        "  with open(file_path3, 'r') as file:\n",
        "      text3 = file.read()\n",
        "\n",
        "  text1=text1[2:]\n",
        "  text2=text2[2:]\n",
        "  text3=text3[2:]\n",
        "\n",
        "\n",
        "  text1=text1.split(':$')\n",
        "  text2=text2.split(':$')\n",
        "  text3=text3.split(':$')\n",
        "\n",
        "  ls=[]\n",
        "\n",
        "  summary_generator(text1, 1)\n",
        "  summary_generator(text2, 2)\n",
        "  summary_generator(text3, 3)\n",
        "\n",
        "  final_summary_good = ' '.join(final_summary_good)\n",
        "  final_summary_bad = ' '.join(final_summary_bad)\n",
        "  c = ' '.join(sentiment_summary)\n",
        "\n",
        "  from google.colab import drive\n",
        "  from transformers import pipeline\n",
        "\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  # Setup paths and summarizer\n",
        "  file_path1 = '/content/summary1.txt'\n",
        "  file_path2 = '/content/summary2.txt'\n",
        "  file_path3 = '/content/summary3.txt'\n",
        "  summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "  # Function to generate summaries\n",
        "  def summary_generator(text, array, n, array1=None, array2=None):\n",
        "      for i in text:\n",
        "          summary = summarizer(i, max_length=130, min_length=30, do_sample=False)\n",
        "          if n != 1:\n",
        "              array += ' ' + summary[0]['summary_text']\n",
        "          else:\n",
        "              array1.append(summary[0]['summary_text'])\n",
        "              array2.append(i)\n",
        "\n",
        "  # Read files and split texts\n",
        "  def read_and_split(file_path):\n",
        "      with open(file_path, 'r') as file:\n",
        "          return file.read().split(':$')\n",
        "\n",
        "  text1 = read_and_split(file_path1)\n",
        "  text2 = read_and_split(file_path2)\n",
        "  text3 = read_and_split(file_path3)\n",
        "\n",
        "  # Initialize variables for summaries\n",
        "  final_summary_good = ''\n",
        "  final_summary_bad = ''\n",
        "  sentiment_summary = []\n",
        "  sentiment_input = []\n",
        "  ls = []\n",
        "\n",
        "  # Generate summaries\n",
        "  summary_generator(text1, final_summary_good, 0)\n",
        "  summary_generator(text2, final_summary_bad, 0)\n",
        "  summary_generator(text3, ls, 1, sentiment_summary, sentiment_input)\n",
        "\n",
        "\n",
        "  def sentiment(input):\n",
        "    checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "    inputs = tokenizer(input, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = tf.nn.softmax(logits)\n",
        "\n",
        "    predicted_class_id = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "    class_names = ['negative', 'positive']\n",
        "    predicted_class_name = class_names[predicted_class_id]\n",
        "\n",
        "    return predicted_class_name\n",
        "\n",
        "  for i in range(len(sentiment_input)):\n",
        "    if(sentiment(sentiment_input[i])=='positive'):\n",
        "        final_summary_good+=(' ' + sentiment_summary[i])\n",
        "\n",
        "    else:\n",
        "        final_summary_bad+=(' ' + sentiment_summary[i])\n",
        "\n",
        "\n",
        "  # This is the model for the title generation\n",
        "  from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "  def title(text):\n",
        "      # Load the model and tokenizer for TensorFlow\n",
        "      model_name = \"t5-base\"\n",
        "      tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "      model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "      # Prepare the text input by adding the prefix \"summarize: \"\n",
        "      input_text = f\"summarize: {text}\"\n",
        "      inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=512, truncation=True)\n",
        "\n",
        "      # Generate output using the model\n",
        "      summary_ids = model.generate(inputs['input_ids'], num_beams=4, no_repeat_ngram_size=2, min_length=5, max_length=20)\n",
        "      summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "      return summary\n",
        "\n",
        "  title1 = title(final_summary_good)\n",
        "  title2 = title(final_summary_bad)\n",
        "\n",
        "  summary_dict={'title_good':title1,'title_bad':title2,'good_summary':final_summary_good,'bad_summary':final_summary_bad }\n",
        "\n",
        "  # filename = 'summary.json'\n",
        "\n",
        "  # # Write the dictionary to the file as JSON\n",
        "  # with open(filename, 'w') as file:\n",
        "  #     json.dump(summary_dict, file, indent=4)\n",
        "\n",
        "  from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "  from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
        "\n",
        "  text=final_summary_good + final_summary_bad\n",
        "\n",
        "  def split_into_word_sets(text, max_words=10):\n",
        "      import re\n",
        "      # Split text into sentences\n",
        "      sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "\n",
        "      sets = []\n",
        "      current_set = []\n",
        "      current_word_count = 0\n",
        "\n",
        "      for sentence in sentences:\n",
        "          # Count words in the sentence\n",
        "          words_in_sentence = sentence.split()\n",
        "\n",
        "          # Check if adding this sentence would exceed the max words\n",
        "          if current_word_count + len(words_in_sentence) > max_words:\n",
        "              # If so, start a new set\n",
        "              sets.append(\" \".join(current_set))\n",
        "              current_set = words_in_sentence\n",
        "              current_word_count = len(words_in_sentence)\n",
        "          else:\n",
        "              # Otherwise, add the words to the current set\n",
        "              current_set.extend(words_in_sentence)\n",
        "              current_word_count += len(words_in_sentence)\n",
        "\n",
        "      # Add the last set if it's not empty\n",
        "      if current_set:\n",
        "          sets.append(\" \".join(current_set))\n",
        "\n",
        "      return sets\n",
        "\n",
        "\n",
        "  ls=[]\n",
        "\n",
        "  ls= split_into_word_sets(text,50)\n",
        "\n",
        "  model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "  pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "\n",
        "  def postag(text):\n",
        "    result = pipeline(text)\n",
        "    return result\n",
        "\n",
        "  phrases=[]\n",
        "  final=[]\n",
        "\n",
        "  def phraseadder(result):\n",
        "    for i in range(len(result)-1):\n",
        "      if(result[i]['entity']=='JJ'):\n",
        "        word=result[i]['word']\n",
        "        if '#' not in word:\n",
        "          phrases.append(word)\n",
        "\n",
        "  for i in ls:\n",
        "    final.append(postag(i))\n",
        "\n",
        "  for i in final:\n",
        "    phraseadder(i)\n",
        "\n",
        "  print(phrases)\n",
        "\n",
        "  from collections import Counter, defaultdict\n",
        "  import nltk\n",
        "  from nltk.corpus import wordnet\n",
        "\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "  def is_word_in_wordnet(word):\n",
        "      return bool(wordnet.synsets(word))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def group_words_by_frequency(words):\n",
        "      # Count the frequencies of each word\n",
        "      frequency = Counter(words)\n",
        "\n",
        "      blacklist=['first','second','third','previous','sequel','prequel','other','many','same','similar','non']\n",
        "      # Group words by their frequency\n",
        "      frequency_groups = defaultdict(list)\n",
        "      for word, count in frequency.items():\n",
        "          if(word not in blacklist):\n",
        "            frequency_groups[count].append(word)\n",
        "\n",
        "      return frequency_groups\n",
        "\n",
        "  # Example usage:\n",
        "  words = phrases\n",
        "  grouped = group_words_by_frequency(words)\n",
        "\n",
        "  # Print the groups\n",
        "  for freq, words in sorted(grouped.items()):\n",
        "      print(f\"Frequency {freq}: {words}\")\n",
        "\n",
        "  for i in grouped:\n",
        "    for j in grouped[i]:\n",
        "      if(not is_word_in_wordnet(j)):\n",
        "        grouped[i].remove(j)\n",
        "\n",
        "  # filename = 'adjectives.json'\n",
        "\n",
        "  # Write the dictionary to the file as JSON\n",
        "  # with open(filename, 'w') as file:\n",
        "  #     json.dump(grouped, file, indent=4)\n",
        "  print(summary_dict)\n",
        "  return {'adjectives':grouped,'summaries':summary_dict}\n",
        "\n",
        "main()"
      ]
    }
  ]
}