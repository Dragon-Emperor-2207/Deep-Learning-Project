{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y_BepRYyUAsg"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM, TFAutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Assuming the file path within your Google Drive is 'Colab Notebooks/MyFile.txt'\n",
        "file_path1 = '/content/summary1.txt'\n",
        "file_path2 = '/content/summary2.txt'\n",
        "file_path3 = '/content/summary3.txt'\n",
        "\n",
        "final_summary_good = []\n",
        "final_summary_bad = []\n",
        "neutral_sammary = ''\n",
        "\n",
        "sentiment_input = []\n",
        "sentiment_summary = []\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "def summary_generator(text,n):\n",
        "  global final_summary_good, final_summary_bad, sentiment_summary, sentiment_input\n",
        "\n",
        "  array= ''\n",
        "  for i in text:\n",
        "    summary = summarizer(i, max_length=130, min_length=30, do_sample=False)\n",
        "    m=summary[0]['summary_text']+ ' '\n",
        "\n",
        "    if n==1:\n",
        "      final_summary_good.append(m)\n",
        "\n",
        "    elif n==2:\n",
        "      final_summary_bad.append(m)\n",
        "\n",
        "    else:\n",
        "      sentiment_summary.append(m)\n",
        "      sentiment_input.append(i)\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "\n",
        "with open(file_path1, 'r') as file:\n",
        "    text1 = file.read()\n",
        "\n",
        "with open(file_path2, 'r') as file:\n",
        "    text2 = file.read()\n",
        "\n",
        "with open(file_path3, 'r') as file:\n",
        "    text3 = file.read()\n",
        "\n",
        "text1=text1[2:]\n",
        "text2=text2[2:]\n",
        "text3=text3[2:]\n",
        "\n",
        "\n",
        "text1=text1.split(':$')\n",
        "text2=text2.split(':$')\n",
        "text3=text3.split(':$')\n",
        "\n",
        "ls=[]\n",
        "\n",
        "summary_generator(text1, 1)\n",
        "summary_generator(text2, 2)\n",
        "summary_generator(text3, 3)\n",
        "\n",
        "print(' '.join(final_summary_good))\n",
        "print(' '.join(final_summary_bad))\n",
        "print(' '.join(sentiment_summary))\n",
        "\n",
        "a = ' '.join(final_summary_good)\n",
        "b = ' '.join(final_summary_bad)\n",
        "c = ' '.join(sentiment_summary)"
      ],
      "metadata": {
        "id": "tLD1x3PVxwux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de655b89-edb9-408f-acd5-00a438f4f564"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All three actresses are good and of course Samuel L in always a pleasure to see . Most of the scenes were just downright silly . I wish Marvel would stop being silly and go back to making movies for adults and kids .  The MARVELS is here and it's a mashed up film that is totally all over the place . I did like seeing Carol Danvers and Monica Rambeau resolve and acknowledge their relationship, but due to the nature of their characters being straight arrows, it allows Kamala Khan to be the audience just as Tom Holland's Peter Parker was in CIVIL WAR .  Carol Danvers is attempting to save the universe, but her incredible powers become entangled with those of Kamala Khan and Monica Rambeau . I honestly don't know who the target audience is, or who this is going to appeal to, young teens only maybe? Anyhow, this was an absolute mess . The writing was clearly problematic, it doesn't gel, it is genuinely all over the place .  There are 0(zero) good movies and only 1 or 2 tv shows that are watchable over a total of 20 productions and this is one of the bottom ranked ones . Marvel did a fantastic job over the decade, earned trust of fans besides billions of dollars .  Post Avengers Endgame, these projects have been feeling more and more like content than cinema (except the rare surprises like GOTG 3). There's nothing new for me to say that I haven't already said about any of the other MCU content as of late . With every new MCU project, the jokes-per-minute ratio keeps getting worse, the villains are a writer's version of a stock photograph .  \"The Marvels\" is the sequel to 2019's \"Captain Marvel\" and the 33rd film in the Marvel Cinematic Universe . It's becoming harder and harder to call myself fan of the MCU, given the array of serviceable to terrible projects they have been churning out lately . I'm not sure whether it's Brie Larson's underwhelming performance or just plain old bad writing .  2019's \"Captain Marvel\" wasn't too bad for a superhero movie, but its sequel \"The Marvels\" is an unmitigated stinking disaster . Director Nia DaCosta creates a headache inducing, fast (as if bizarrely sped up), frantic, thoroughly annoying, tiresome piece of garbage .  I am honestly happy this movie did bad at the box office . What makes it bad: Action moves way too fast from one scene to another . Nothing serious enough, too childish for my taste .  This film is somehow way more depressing and worse than Ant Man Quantumania . It felt like watching a beloved friend in a coma fighting for her life . There is no chemistry between the two at all . The villain is Bland with a capitol B, and highly forgettable .  This second installment makes the original Captain Marvel look like a cinematic masterpiece... Nothing about this movie works . In fact, when you break it down, there is really no story at all happening . The solution to the entire problem is so extremely simple and solvable . \n",
            "It's not \"a work of art\" but I enjoyed it much more than a lot of recent Marvel movies . The three main characters are delightful - and I had seen WandaVision and Ms Marvel to understand who Monica Rambeau and Kamala Khan were .  I had hoped that following Guardians 3 it could've got much worse for the Marvel universe, at least Guardians has Rocket It's hard to find any plot to this movie . The movie is blissfully short but feels longer as it's so bad and some people walked out of the theatre .  I just watched it on Disney+ now that it's available and it wasn't terrible at all . I didn't get around to seeing it in theaters . It was more childish at times (following the theme of the show that includes the same child main character)  I thought that this film was pretty good, though I have to agree that the problem is that you do need to catch up to speed to understand what is going on . Mind you, I did actually quite like the film. Like the scenes where they were switching places whenever they used their powers together, and also the big bad in the film, that was in part trying to save her people .  I avoided the movie for over half a year due to the negative reviews and my own marvel-burnout . I say this to establish how low my expectations were . This was never going for an oscar or trying to usurp Infinity War in terms of \"depth\"  My biggest challenge were equally two of my favorite things in the film . For those who don't watch Disney+ two (more actually) of the primary characters were unknown to theatre audiences . Somewhere in here was a much stronger film than we, the audience, were able to see .  Overall I enjoyed the movie and found it a decent watch enjoyable superhero entertainer movie . The three main cast of the movie are the highlight and their bonding is superb . Iman Vellani as Kamala Khan (Ms Marvel) is by far the best portayed character ever and does a great job once again doing that character well .  I really don't understand the hate this movie received . It is a fun romp through the universe with great action, decent humor and some really heart felt moments . We see more emotion from a Carol who's not the cocksure pilot from the first movie .  I would have skipped this one if the trailer didn't look like so much fun . The movie leans heavily - and for the most part successfully - into the comedy . It is fueled by the teenage Captain Marvel fangirl Ms. Marvel .  This movie has serious cult film potential . If you haven't seen Ms Marvel, Captain Marvel and Wandavision, it will make very little sense . Beyond this, the flerkens had me laughing until I cried .  I really enjoyed this movie better than a lot of the recent superhero movies . I noticed some were moaning about Miss Marvel being juvenile but that's the point of her she's a teenager .  I believe the movie is unfairly treated and that included me as well . The trailers don't do this movie much justice and the marketing was also poor .  Iman Vellani continues to be a delight, with Kamala Khan's character and family providing both the grounded heart of the film and many of its funniest moments . The villain of the piece is one of the least notable Marvel antagonists so far . \n",
            "The storyline lacked any real tension, but I didn't really care . In a lot of ways, it is a set up for the rest of the series, and definitely not a standalone movie .  This is a fun movie, it ties two TV shows into the lore and does a good job of having fun . If you get over its not adding anything or revealing any easter eggs, you can enjoy it for what it is . It improves on captain marvels solo outing . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkvMoETb5yTZ",
        "outputId": "d8f5d89e-1e9a-4637-c4fa-52a9cb7c8e9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F00qU9ghxv85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import pipeline\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Setup paths and summarizer\n",
        "file_path1 = '/content/summary1.txt'\n",
        "file_path2 = '/content/summary2.txt'\n",
        "file_path3 = '/content/summary3.txt'\n",
        "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
        "\n",
        "# Function to generate summaries\n",
        "def summary_generator(text, array, n, array1=None, array2=None):\n",
        "    for i in text:\n",
        "        summary = summarizer(i, max_length=130, min_length=30, do_sample=False)\n",
        "        if n != 1:\n",
        "            array += ' ' + summary[0]['summary_text']\n",
        "        else:\n",
        "            array1.append(summary[0]['summary_text'])\n",
        "            array2.append(i)\n",
        "\n",
        "# Read files and split texts\n",
        "def read_and_split(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read().split(':$')\n",
        "\n",
        "text1 = read_and_split(file_path1)\n",
        "text2 = read_and_split(file_path2)\n",
        "text3 = read_and_split(file_path3)\n",
        "\n",
        "# Initialize variables for summaries\n",
        "final_summary_good = ''\n",
        "final_summary_bad = ''\n",
        "sentiment_summary = []\n",
        "sentiment_input = []\n",
        "ls = []\n",
        "\n",
        "# Generate summaries\n",
        "summary_generator(text1, final_summary_good, 0)\n",
        "summary_generator(text2, final_summary_bad, 0)\n",
        "summary_generator(text3, ls, 1, sentiment_summary, sentiment_input)\n",
        "\n",
        "# Output results (for checking)\n",
        "print(final_summary_good)\n",
        "print(final_summary_bad)\n",
        "print(sentiment_summary)\n"
      ],
      "metadata": {
        "id": "pCfnPLzqVT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27861256-1d31-4f7a-b2b7-c6a3dffa99ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 512). Running this sequence through the model will result in indexing errors\n",
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
            "Your max_length is set to 130, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "[': . :: ! !! .. .! ? !. & ! # !', \"The storyline lacked any real tension, but I didn't really care . In a lot of ways, it is a set up for the rest of the series, and definitely not a standalone movie .\", 'This is a fun movie, it ties two TV shows into the lore and does a good job of having fun . If you get over its not adding anything or revealing any easter eggs, you can enjoy it for what it is . It improves on captain marvels solo outing .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(input):\n",
        "  checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "  model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "  inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True, max_length=512)\n",
        "\n",
        "  outputs = model(inputs)\n",
        "  logits = outputs.logits\n",
        "  predictions = tf.nn.softmax(logits)\n",
        "\n",
        "  predicted_class_id = tf.argmax(predictions, axis=1).numpy()[0]\n",
        "  class_names = ['negative', 'positive']\n",
        "  predicted_class_name = class_names[predicted_class_id]\n",
        "\n",
        "  return predicted_class_name\n",
        "\n",
        "for i in range(len(sentiment_input)):\n",
        "  if(sentiment(sentiment_input[i])=='positive'):\n",
        "      final_summary_good+=(' ' + sentiment_summary[i])\n",
        "\n",
        "  else:\n",
        "      final_summary_bad+=(' ' + sentiment_summary[i])\n"
      ],
      "metadata": {
        "id": "1pqm2zGF01aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the model for the title generation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"czearing/article-title-generator\")\n",
        "model = TFAutoModelForCausalLM.from_pretrained(\"czearing/article-title-generator\")\n",
        "\n",
        "def title(input):\n",
        "\n",
        "  inputid = tokenizer.endcode(input,return_tensors='tf')\n",
        "\n",
        "  encodedresult = model.generate(inputid,max_length= 200)\n",
        "\n",
        "  decodedresult = tokenizer.decode(encodedresult[0], skip_special_tokens=True)\n",
        "\n",
        "  return decodedresult\n",
        "\n",
        "title1 = title(final_summary_good)\n",
        "title2 = title(final_summary_bad)"
      ],
      "metadata": {
        "id": "eu-puI4cagWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Assuming the file path within your Google Drive is 'Colab Notebooks/MyFile.txt'\n",
        "file_path = '/content/sumary.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "def split_into_word_sets(text, max_words=10):\n",
        "    import re\n",
        "    # Split text into sentences\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "\n",
        "    sets = []\n",
        "    current_set = []\n",
        "    current_word_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Count words in the sentence\n",
        "        words_in_sentence = sentence.split()\n",
        "\n",
        "        # Check if adding this sentence would exceed the max words\n",
        "        if current_word_count + len(words_in_sentence) > max_words:\n",
        "            # If so, start a new set\n",
        "            sets.append(\" \".join(current_set))\n",
        "            current_set = words_in_sentence\n",
        "            current_word_count = len(words_in_sentence)\n",
        "        else:\n",
        "            # Otherwise, add the words to the current set\n",
        "            current_set.extend(words_in_sentence)\n",
        "            current_word_count += len(words_in_sentence)\n",
        "\n",
        "    # Add the last set if it's not empty\n",
        "    if current_set:\n",
        "        sets.append(\" \".join(current_set))\n",
        "\n",
        "    return sets\n",
        "\n",
        "\n",
        "ls=[]\n",
        "\n",
        "ls= split_into_word_sets(text,50)\n",
        "\n",
        "model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "\n",
        "def postag(text):\n",
        "  result = pipeline(text)\n",
        "  return result\n",
        "\n",
        "phrases=[]\n",
        "final=[]\n",
        "\n",
        "def phraseadder(result):\n",
        "  global phrases\n",
        "  for i in range(len(result)-1):\n",
        "    if(result[i]['entity']=='JJ'):\n",
        "      word=result[i]['word']\n",
        "      if '#' not in word:\n",
        "        phrases.append(word)\n",
        "\n",
        "for i in ls:\n",
        "  final.append(postag(i))\n",
        "\n",
        "for i in final:\n",
        "  phraseadder(i)\n",
        "\n",
        "print(phrases)\n"
      ],
      "metadata": {
        "id": "_Dj7XlNlRGne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list phrases will contain adjectives followed by nouns. The enxt task would be to group together phrases that are similar."
      ],
      "metadata": {
        "id": "Fw7Wold0TTDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def group_words_by_frequency(words):\n",
        "    # Count the frequencies of each word\n",
        "    frequency = Counter(words)\n",
        "\n",
        "    blacklist=['first','second','third','previous','sequel','prequel','other','many','same','similar','non']\n",
        "    # Group words by their frequency\n",
        "    frequency_groups = defaultdict(list)\n",
        "    for word, count in frequency.items():\n",
        "        if(word not in blacklist):\n",
        "          frequency_groups[count].append(word)\n",
        "\n",
        "    return frequency_groups\n",
        "\n",
        "# Example usage:\n",
        "words = phrases\n",
        "grouped = group_words_by_frequency(words)\n",
        "\n",
        "# Print the groups\n",
        "for freq, words in sorted(grouped.items()):\n",
        "    print(f\"Frequency {freq}: {words}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "5IYlNT8TTSvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}